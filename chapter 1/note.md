







## Bias를 왜 사용 하는가
	•	역할: 뉴런이 단순히 입력 × 가중치의 선형결합에만 의존하지 않고, 학습 과정에서 평행 이동(shift)을 할 수 있게 해줍니다.
	•	비유: y = w·x + b 에서 기울기(w)만 있으면 원점(0,0)을 반드시 지나야 하지만, b가 있으면 직선을 위아래로 이동시켜 더 다양한 함수 표현이 가능해집니다.
	•	효과: 모델이 더 복잡한 패턴을 학습할 수 있도록 유연성을 줍니다.

## 활성화 함수를 왜 사용하는가?

	•	역할: 단순한 선형 결합 값을 비선형으로 변환하여, 신경망이 복잡한 함수(곡선, 경계, 특징)를 표현할 수 있도록 합니다.
	•	비선형성 추가: 활성화 함수가 없다면 여러 층을 쌓아도 결국 하나의 선형 변환과 같아집니다.
	•	대표 함수:
	•	Sigmoid: 확률처럼 0~1 사이 값으로 압축.
	•	ReLU: 0보다 작으면 0, 크면 그대로 통과 → 계산 효율적, 딥러닝에서 자주 사용.
	•	Softmax: 분류 문제에서 확률 분포로 변환.



## 드롭아웃을 왜 사용하는가

	•	역할: 학습 중 일부 뉴런을 랜덤하게 꺼서(overfitting 방지) 특정 뉴런에만 의존하지 않도록 만듭니다.
	•	효과:
	•	모델이 더 일반화된 패턴을 학습.
	•	과적합 줄이고, 테스트 데이터에 대한 성능 향상.
	•	비유: 팀플할 때 특정 사람만 맨날 일하면 편향되니까, 랜덤으로 빠지게 해서 모두 고르게 훈련시키는 느낌.


## 활성화 함수에서 나온 값은 무조건 다음 뉴런으로 전파됨?

	•	원칙적으로는 예.
각 뉴런의 출력(= 활성화 함수 결과)은 다음 층 뉴런들의 입력이 됩니다.
	•	예외적 상황:
	•	드롭아웃이 걸려서 일부 뉴런이 “꺼져” 있을 때는 전파되지 않습니다.
	•	학습 중 특정 활성화 함수(ReLU 등)에서 출력이 0이 되면, 그 뉴런은 해당 스텝에서 사실상 아무 신호도 전달하지 않습니다.
	•	따라서 “무조건” 전파된다기보다는, 활성화된 뉴런의 출력만 전파된다고 보는 게 맞습니다.


소프트맥스 함수는 출력 벡터 전체에 대해 동시에 계산됩니다.
즉, 각 출력 뉴런마다 독립적으로 sigmoid처럼 따로따로 계산하는 게 아니라, 모든 출력 값의 합이 반드시 1이 되도록 정규화합니다.

## Data-Set 종류

### train
- 학습에 사용되는 데이터

### val
- 매 이폭(epochs)마다 학습된 몯레을 평가하기 위한 데이터셋, val이 없을 경우 train으로 매 이폭마다 평가

### test
- 학습이 완료된 모델의 평가를 위한 데이터셋

```
[참고] 학습 중 모델을 평가할 때의 데이터 셋
- accuracy, loss --> train
- val_accuracy, val_loss --> val
```


원핫 인코딩

레이블 인코딩

임베딩 --> 주로 입력 데이터를 벡터 공간에 표현하는데 쓰임

	•	원-핫 인코딩 → 손실 함수(CrossEntropy Loss)에서 직접적으로 사용.
	•	레이블 인코딩 → 손실 함수에서 내부적으로 원-핫으로 바꿔 사용.
	•	임베딩 → 입력 표현 방식, 손실 함수는 출력 비교에서 사용.

1. 원핫 인코딩이란
- 출력층에서 나온 값을 정답 데이터와 어떻게 비교하는가?

2. 다중 분류 문제에서 출력층에 softmax 활성화 함수를 사용하는 이유




가중치 업데이트를 순전파, 역전파 두 방식이 있는데 대부분 역전파 방식 사용함

순전파를 feed forward neural network

역전파를 back propagation 이라고 함


일반적으로 gradient exploding이 일어날 떄, 학습률을 작게 조정한다 (Small Learning Rate) 

분류를 위한 출력층에서 쓰이는 활성화 함수 -> 소프트 맥스
회귀 모형에서 주로 쓰는 활성화 함수 —> 시그모이드
이진 분류 —> tanh ?